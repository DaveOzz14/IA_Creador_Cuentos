Generador de Texto con LMStudio y DeepSeek LLM

Este es un proyecto simple en Python que utiliza LMStudio y el modelo deepseek-r1-distill-llama-8b para generar texto a partir de un prompt.

ğŸš€ Requisitos

Python 3.8+

LMStudio instalado y ejecutÃ¡ndose localmente

Modelo deepseek-r1-distill-llama-8b cargado en LMStudio

Dependencias Python:

pip install requests

ğŸ“Œ InstalaciÃ³n y ConfiguraciÃ³n

1ï¸âƒ£ Descarga e instala LMStudio

LMStudio se puede descargar desde:
ğŸ”— https://lmstudio.ai/

2ï¸âƒ£ Carga el modelo en LMStudio

Abre LMStudio y carga el modelo deepseek-r1-distill-llama-8b.

3ï¸âƒ£ Ejecuta el servidor local

AsegÃºrate de que LMStudio estÃ¡ corriendo en http://localhost:1234. Puedes verificarlo accediendo a:

http://localhost:1234/v1/models

DeberÃ­as ver el modelo cargado en la respuesta JSON.

ğŸ“ Uso

1ï¸âƒ£ Clona este repositorio:

git clone https://github.com/tu-usuario/tu-repo.git
cd tu-repo

2ï¸âƒ£ Ejecuta el script:

python main.py

ğŸ›  CÃ³digo Principal (main.py)

import requests

LMSTUDIO_URL = "http://localhost:1234/v1/completions"

def generar_texto(prompt):
    payload = {
        "model": "deepseek-r1-distill-llama-8b",
        "prompt": prompt,
        "temperature": 0.7,
        "max_tokens": 300
    }
    response = requests.post(LMSTUDIO_URL, json=payload)
    response_json = response.json()
    return response_json.get("choices", [{}])[0].get("text", "Error: Sin respuesta")

print(generar_texto("Ã‰rase una vez un castillo encantado..."))

ğŸ¯ Contribuciones

Â¡Las contribuciones son bienvenidas! Puedes hacer un fork, crear una branch y enviar un pull request.

ğŸ“œ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Â¡Ãšsalo libremente! ğŸš€




