import requests

# Endpoint local de LMStudio
LMSTUDIO_URL = "http://localhost:1234/v1/completions"  # ğŸ‘ˆ Usa /v1/completions

def generar_texto(prompt):
    """EnvÃ­a el prompt al LLM local y devuelve la respuesta generada."""
    payload = {
        "model": "deepseek-r1-distill-llama-8b",
        "prompt": prompt,  # ğŸ‘ˆ Cambia 'messages' por 'prompt'
        "temperature": 0.7,
        "max_tokens": 300
    }

    response = requests.post(LMSTUDIO_URL, json=payload)

    try:
        response_json = response.json()
        print("ğŸ” Respuesta de LMStudio:", response_json)  # ğŸ‘ˆ Imprime para depurar
        
        # Verifica si "choices" existe en la respuesta
        if "choices" in response_json and len(response_json["choices"]) > 0:
            return response_json["choices"][0]["text"]  # ğŸ‘ˆ Cambia "message" por "text"
        else:
            return "âš ï¸ Error: No se encontraron respuestas vÃ¡lidas en LMStudio."
    
    except Exception as e:
        return f"âŒ Error al procesar la respuesta: {str(e)}"

# Prueba rÃ¡pida
print(generar_texto("Eres experto en crear cuentos infantiles. Vas a Crear un cuento de 100 palabras inciando con este texto: 'Ã‰rase una vez un castillo encantado...'"))

